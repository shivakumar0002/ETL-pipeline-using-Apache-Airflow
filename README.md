

# ğŸŒ¦ï¸ ETL Pipeline using Apache Airflow

This project demonstrates a complete end-to-end ETL (Extract, Transform, Load) pipeline using **Apache Airflow**, **PostgreSQL**, and **Open-Meteo API**, all containerized with **Docker**.

---

## ğŸš€ What This Project Does

- ğŸ”„ **Extract**: Weather data from the Open-Meteo API
- ğŸ”§ **Transform**: Formats the JSON response into clean tabular data
- ğŸ—„ï¸ **Load**: Inserts transformed data into a PostgreSQL database
- ğŸ“… **Schedules**: Runs daily using Airflow's scheduler

---

## ğŸ› ï¸ Tech Stack

- ğŸ§¬ Apache Airflow
- ğŸ˜ PostgreSQL (Dockerized)
- ğŸ³ Docker + Docker Compose
- â˜ï¸ Open-Meteo API
- ğŸ§‘â€ğŸ’» Python

---

## ğŸ“‚ Project Structure

```bash
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etlweather.py          # The main DAG file
â”œâ”€â”€ docker-compose.yml         # Local container orchestration
â”œâ”€â”€ requirements.txt           # Python dependencies (if needed)
â”œâ”€â”€ README.md                  # Youâ€™re looking at it 

âš™ï¸ How to Run Locally
	1.	Start Docker
	2.	Run Airflow with Astro CLI 
    ```bash
    astro dev start
    ```
    3.	Open Airflow UI: http://localhost:8080
	â€¢	Username: admin
	â€¢	Password: admin
	4.	Trigger the DAG: weather_etl_pipeline 

ğŸ” Verify in Postgres

Connect via any client like DBeaver: 
``` bash
Host: localhost
Port: 5432
User: postgres
Password: postgres
Database: postgres
```

Check data:
```bash
SELECT * FROM weather;
```
ğŸ§  What Youâ€™ll Learn
	â€¢	How to structure a real-world ETL pipeline
	â€¢	Using Airflowâ€™s task decorators and hooks
	â€¢	Managing containers and services via Docker
	â€¢	Debugging and developing in an MLOps-style workflow 

    
## ğŸ–¼ï¸ DAG Screenshot

Hereâ€™s what the pipeline looks like in the Airflow UI:

![DAG Screenshot](assets/dag_screenshot.png) 
